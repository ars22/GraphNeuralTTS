{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scripts to analyze audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR=\"/usr0/home/amadaan/audio/tacotron_baseline/Tacotron-pytorch/training-adb/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-spectogram classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_2d(x, max_len):\n",
    "    x = np.pad(x, [(0, max_len - len(x)), (0, 0)],\n",
    "               mode=\"constant\", constant_values=0)\n",
    "    return x\n",
    "\n",
    "class MelDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pth):\n",
    "        self.mel_files = glob.glob(f\"{BASEDIR}/*mel*\")\n",
    "        print(f\"{len(self.mel_files)} mel-files found\")\n",
    "        # the files are supposed to be named accent_speaker_*.wav, \n",
    "        # e.g. australian_s02_362.wav\n",
    "        self.labels = [os.path.basename(mel_file_pth).split(\"_\")[:2] for mel_file_pth in self.mel_files]\n",
    "        labels = [os.path.basename(mel_file_pth).split(\"_\")[:2] for mel_file_pth in self.mel_files]\n",
    "        \n",
    "        # get accent labels, make a dict\n",
    "        self.accent_labels = [l[0] for l in self.labels]\n",
    "        self.accent_label_dict = {k: i for i, k in enumerate(sorted(Counter(self.accent_labels).keys()))}\n",
    "        print(self.accent_label_dict)\n",
    "        \n",
    "        # same processing for the speakers\n",
    "        self.speaker_labels = [\" \".join(l) for l in self.labels]\n",
    "        self.speaker_label_dict = {k: i for i, k in enumerate(sorted(Counter(self.speaker_labels).keys()))}\n",
    "        print(self.speaker_label_dict)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return np.load(self.mel_files[i])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mel_files)\n",
    "\n",
    "    @staticmethod\n",
    "    def batchify(dataset, bsz, shuffle=True):\n",
    "        idx = list(range(len(dataset)))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx)\n",
    "\n",
    "        for begin in range(0, len(dataset), bsz):\n",
    "            end = min(begin + bsz, len(dataset))\n",
    "            num_elems = end - begin\n",
    "    \n",
    "            \n",
    "            # read all the mels for this batch, find the max length\n",
    "            mels = [dataset[idx[i]] for i in range(begin, end)]\n",
    "            seq_lengths = torch.LongTensor([len(mel) for mel in mels])\n",
    "            max_target_len = seq_lengths.max().item()\n",
    "            \n",
    "            \n",
    "            b = np.array([_pad_2d(mel, max_target_len) for mel in mels],\n",
    "                 dtype=np.float32)\n",
    "            mel_batch = torch.FloatTensor(b)\n",
    "            speaker_labels = torch.LongTensor([dataset.speaker_label_dict[dataset.speaker_labels[idx[i]]]\\\n",
    "                                               for i in range(begin, end)])\n",
    "            accent_labels = torch.LongTensor([dataset.accent_label_dict[dataset.accent_labels[idx[i]]]\\\n",
    "                                              for i in range(begin, end)])\n",
    "            \n",
    "            \n",
    "            seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "            \n",
    "            \n",
    "            yield mel_batch[perm_idx], speaker_labels[perm_idx], accent_labels[perm_idx], seq_lengths\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset sanity checks\n",
    "dataset = MelDataset(BASEDIR)\n",
    "\n",
    "# check shape of one mel file\n",
    "print(dataset[0].shape)\n",
    "\n",
    "# dataloader check\n",
    "dataloader = MelDataset.batchify(dataset, 32)\n",
    "\n",
    "# check two batches for correct batchification:\n",
    "for _ in range(2):\n",
    "    mel, speaker, accent, input_lengths = next(dataloader)\n",
    "    print(mel.shape, speaker.shape, accent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_class,\n",
    "                 mel_spectogram_dim: int = 80,\n",
    "                 gru_hidden_size=32,\n",
    "                 gru_num_layers=2):\n",
    "        super(MelClassifier, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "                              nn.Conv1d(in_channels=mel_spectogram_dim, out_channels=64, kernel_size=3),\n",
    "                              nn.ELU(),\n",
    "                              nn.BatchNorm1d(64),\n",
    "                              nn.MaxPool2d((2, 2)),\n",
    "                              nn.Dropout(p=0.1))\n",
    "        self.gru = nn.GRU(input_size=32, hidden_size=gru_hidden_size, num_layers=gru_num_layers,\\\n",
    "                          bidirectional=True, batch_first=True, dropout=0.3)\n",
    "        num_directions = 2\n",
    "        self.mlp = nn.Linear(gru_hidden_size * gru_num_layers * num_directions, self.num_class)\n",
    "\n",
    "    def forward(self, mel_batch, input_lengths):\n",
    "        batch_size = len(mel_batch)\n",
    "        # mel_batch -> (batch_size, max_time_step, 80)\n",
    "        conv_output = self.conv_block_1(mel_batch.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # conv_output -> (batch_size, max_time_step, 32)\n",
    "        conv_output = nn.utils.rnn.pack_padded_sequence(\n",
    "                    conv_output, input_lengths, batch_first=True)\n",
    "        output, h_n = self.gru(conv_output)\n",
    "        # h_n -> (4, batch_size, 32)\n",
    "        \n",
    "        h_n = h_n.permute(1, 0, 2).reshape(batch_size, -1)\n",
    "        return self.mlp(h_n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MelClassifier(len(dataset.accent_label_dict)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3,\n",
    "                                    betas=(0.9, 0.99),\n",
    "                                    eps=1e-6,\n",
    "                                    weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "dataset = MelDataset(BASEDIR)\n",
    "\n",
    "dataloader = MelDataset.batchify(dataset, 32)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (mels, speakers, accents) in enumerate(dataloader):\n",
    "        mels = mels.to(device)\n",
    "        speakers = speakers.to(device)\n",
    "        accents = accents.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(mels)\n",
    "        loss = loss_func(logits, accents).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch = {epoch} iter = {i} Loss = {round(np.array(losses).mean(), 2)}\")\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block_1 = nn.Sequential(\n",
    "                              nn.Conv1d(in_channels=80, out_channels=64, kernel_size=3),\n",
    "                              nn.ELU(),\n",
    "                              nn.BatchNorm1d(64),\n",
    "                              nn.MaxPool2d((2, 2)),\n",
    "                              nn.Dropout(p=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel, speaker, accent, input_lengths = next(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = conv_block_1(mel.permute(0, 2, 1)).permute(0, 2, 1); conv_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_conv_output = nn.utils.rnn.pack_padded_sequence(\n",
    "                    conv_output, input_lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_conv_output.batch_sizes.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(input_lengths).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_conv_output.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size=32, hidden_size=32, num_layers=2,\\\n",
    "                          bidirectional=True, batch_first=True, dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_conv_output.batch_sizes.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, h_n = gru(packed_conv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
