{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scripts to analyze audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR=\"/usr2/asetlur/GraphNeuralTTS/Tacotron-pytorch/training-accentdb-char-baseline-with-additional-info/\"\n",
    "LOGDIR=\"log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-spectogram classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_2d(x, max_len):\n",
    "    x = np.pad(x, [(0, max_len - len(x)), (0, 0)],\n",
    "               mode=\"constant\", constant_values=0)\n",
    "    return x\n",
    "\n",
    "class MelDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pth):\n",
    "        self.mel_files = glob.glob(f\"{BASEDIR}/*mel*\")\n",
    "        print(f\"{len(self.mel_files)} mel-files found\")\n",
    "        # the files are supposed to be named accent_speaker_*.wav, \n",
    "        # e.g. australian_s02_362.wav\n",
    "        self.labels = [os.path.basename(mel_file_pth).split(\"_\")[:2] for mel_file_pth in self.mel_files]\n",
    "        labels = [os.path.basename(mel_file_pth).split(\"_\")[:2] for mel_file_pth in self.mel_files]\n",
    "        \n",
    "        # get accent labels, make a dict\n",
    "        self.accent_labels = [l[0] for l in self.labels]\n",
    "        self.accent_label_dict = {k: i for i, k in enumerate(sorted(Counter(self.accent_labels).keys()))}\n",
    "        print(self.accent_label_dict)\n",
    "        \n",
    "        # same processing for the speakers\n",
    "        self.speaker_labels = [\" \".join(l) for l in self.labels]\n",
    "        self.speaker_label_dict = {k: i for i, k in enumerate(sorted(Counter(self.speaker_labels).keys()))}\n",
    "        print(self.speaker_label_dict)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return np.load(self.mel_files[i])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mel_files)\n",
    "\n",
    "    @staticmethod\n",
    "    def batchify(dataset, bsz, shuffle=True):\n",
    "        idx = list(range(len(dataset)))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idx)\n",
    "\n",
    "        for begin in range(0, len(dataset), bsz):\n",
    "            end = min(begin + bsz, len(dataset))\n",
    "            num_elems = end - begin\n",
    "    \n",
    "            \n",
    "            # read all the mels for this batch, find the max length\n",
    "            mels = [dataset[idx[i]] for i in range(begin, end)]\n",
    "            seq_lengths = torch.LongTensor([len(mel) for mel in mels])\n",
    "            max_target_len = seq_lengths.max().item()\n",
    "            \n",
    "            \n",
    "            b = np.array([_pad_2d(mel, max_target_len) for mel in mels],\n",
    "                 dtype=np.float32)\n",
    "            mel_batch = torch.FloatTensor(b)\n",
    "            speaker_labels = torch.LongTensor([dataset.speaker_label_dict[dataset.speaker_labels[idx[i]]]\\\n",
    "                                               for i in range(begin, end)])\n",
    "            accent_labels = torch.LongTensor([dataset.accent_label_dict[dataset.accent_labels[idx[i]]]\\\n",
    "                                              for i in range(begin, end)])\n",
    "            \n",
    "            \n",
    "            seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "            \n",
    "            \n",
    "            yield mel_batch[perm_idx], speaker_labels[perm_idx], accent_labels[perm_idx], seq_lengths\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14999 mel-files found\n",
      "{'american': 0, 'australian': 1, 'bangla': 2, 'british': 3, 'indian': 4, 'malayalam': 5, 'odiya': 6, 'telugu': 7, 'welsh': 8}\n",
      "{'american s01': 0, 'american s02': 1, 'american s03': 2, 'american s04': 3, 'american s05': 4, 'american s06': 5, 'american s07': 6, 'american s08': 7, 'australian s01': 8, 'australian s02': 9, 'bangla s01': 10, 'bangla s02': 11, 'british s01': 12, 'british s02': 13, 'indian s01': 14, 'indian s02': 15, 'malayalam s01': 16, 'malayalam s02': 17, 'malayalam s03': 18, 'odiya s01': 19, 'telugu s01': 20, 'telugu s02': 21, 'welsh s01': 22}\n",
      "(154, 80)\n",
      "torch.Size([32, 293, 80]) torch.Size([32]) torch.Size([32])\n",
      "torch.Size([32, 276, 80]) torch.Size([32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# dataset sanity checks\n",
    "dataset = MelDataset(BASEDIR)\n",
    "\n",
    "# check shape of one mel file\n",
    "print(dataset[0].shape)\n",
    "\n",
    "# dataloader check\n",
    "dataloader = MelDataset.batchify(dataset, 32)\n",
    "\n",
    "# check two batches for correct batchification:\n",
    "for _ in range(2):\n",
    "    mel, speaker, accent, input_lengths = next(dataloader)\n",
    "    print(mel.shape, speaker.shape, accent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1dconv(in_channels, out_channels, max_pool=False):\n",
    "    return nn.Sequential(nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1),\n",
    "                      nn.ELU(),\n",
    "                      nn.BatchNorm1d(out_channels),\n",
    "                      nn.MaxPool1d(3, stride=2) if max_pool else nn.Identity(),\n",
    "                      nn.Dropout(p=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_class,\n",
    "                 mel_spectogram_dim: int = 80,\n",
    "                 gru_hidden_size=32,\n",
    "                 gru_num_layers=2):\n",
    "        super(MelClassifier, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "                        get_1dconv(in_channels=mel_spectogram_dim, out_channels=64),\n",
    "                        get_1dconv(in_channels=64, out_channels=128),\n",
    "                        get_1dconv(in_channels=128, out_channels=128, max_pool=True),\n",
    "                        get_1dconv(in_channels=128, out_channels=128, max_pool=True),\n",
    "                        get_1dconv(in_channels=128, out_channels=128, max_pool=True))\n",
    "            \n",
    "        self.gru = nn.GRU(input_size=128, hidden_size=gru_hidden_size, num_layers=gru_num_layers,\\\n",
    "                          bidirectional=True, batch_first=True, dropout=0.3)\n",
    "        num_directions = 2\n",
    "        self.mlp = nn.Linear(gru_hidden_size * gru_num_layers * num_directions, self.num_class)\n",
    "\n",
    "    def forward(self, mel_batch, input_lengths):\n",
    "        batch_size = len(mel_batch)\n",
    "        # mel_batch -> (batch_size, max_time_step, 80)\n",
    "        conv_output = self.conv_blocks(mel_batch.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # conv_output -> (batch_size, max_time_step, 32)\n",
    "\n",
    "        output, h_n = self.gru(conv_output)\n",
    "        # h_n -> (4, batch_size, 32)\n",
    "        \n",
    "        h_n = h_n.permute(1, 0, 2).reshape(batch_size, -1)\n",
    "        return h_n, self.mlp(h_n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MelClassifier(len(dataset.speaker_label_dict)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3,\n",
    "                                    betas=(0.9, 0.99),\n",
    "                                    eps=1e-6,\n",
    "                                    weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = (\"rm\", \"-rf\", f\"{LOGDIR}/*\")\n",
    "subprocess.call(\"%s %s %s\" % args, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14999 mel-files found\n",
      "{'american': 0, 'australian': 1, 'bangla': 2, 'british': 3, 'indian': 4, 'malayalam': 5, 'odiya': 6, 'telugu': 7, 'welsh': 8}\n",
      "{'american s01': 0, 'american s02': 1, 'american s03': 2, 'american s04': 3, 'american s05': 4, 'american s06': 5, 'american s07': 6, 'american s08': 7, 'australian s01': 8, 'australian s02': 9, 'bangla s01': 10, 'bangla s02': 11, 'british s01': 12, 'british s02': 13, 'indian s01': 14, 'indian s02': 15, 'malayalam s01': 16, 'malayalam s02': 17, 'malayalam s03': 18, 'odiya s01': 19, 'telugu s01': 20, 'telugu s02': 21, 'welsh s01': 22}\n",
      "Epoch = 0 iter = 0 Loss = 3.1 Acc = 9.38\n",
      "Epoch = 0 iter = 50 Loss = 2.64 Acc = 25.0\n",
      "Epoch = 0 iter = 100 Loss = 1.86 Acc = 36.14\n",
      "Epoch = 0 iter = 150 Loss = 1.36 Acc = 45.9\n",
      "Epoch = 0 iter = 200 Loss = 0.97 Acc = 53.89\n",
      "Epoch = 0 iter = 250 Loss = 0.67 Acc = 60.27\n",
      "Epoch = 0 iter = 300 Loss = 0.55 Acc = 64.87\n",
      "Epoch = 0 iter = 350 Loss = 0.41 Acc = 68.52\n",
      "Epoch = 0 iter = 400 Loss = 0.33 Acc = 71.59\n",
      "Epoch = 0 iter = 450 Loss = 0.29 Acc = 73.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/468 [00:00<00:12, 38.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:16, 25.39it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 iter = 0 Loss = 0.31 Acc = 74.71\n",
      "Epoch = 1 iter = 50 Loss = 0.22 Acc = 76.63\n",
      "Epoch = 1 iter = 100 Loss = 0.24 Acc = 78.07\n",
      "Epoch = 1 iter = 150 Loss = 0.2 Acc = 79.37\n",
      "Epoch = 1 iter = 200 Loss = 0.18 Acc = 80.56\n",
      "Epoch = 1 iter = 250 Loss = 0.17 Acc = 81.6\n",
      "Epoch = 1 iter = 300 Loss = 0.12 Acc = 82.6\n",
      "Epoch = 1 iter = 350 Loss = 0.13 Acc = 83.42\n",
      "Epoch = 1 iter = 400 Loss = 0.14 Acc = 84.13\n",
      "Epoch = 1 iter = 450 Loss = 0.15 Acc = 84.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/468 [00:00<00:12, 36.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:16, 28.08it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2 iter = 0 Loss = 0.12 Acc = 85.01\n",
      "Epoch = 2 iter = 50 Loss = 0.1 Acc = 85.63\n",
      "Epoch = 2 iter = 100 Loss = 0.11 Acc = 86.18\n",
      "Epoch = 2 iter = 150 Loss = 0.08 Acc = 86.73\n",
      "Epoch = 2 iter = 200 Loss = 0.07 Acc = 87.23\n",
      "Epoch = 2 iter = 250 Loss = 0.08 Acc = 87.67\n",
      "Epoch = 2 iter = 300 Loss = 0.07 Acc = 88.07\n",
      "Epoch = 2 iter = 350 Loss = 0.05 Acc = 88.49\n",
      "Epoch = 2 iter = 400 Loss = 0.11 Acc = 88.77\n",
      "Epoch = 2 iter = 450 Loss = 0.07 Acc = 89.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/468 [00:00<00:20, 22.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:18, 25.65it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3 iter = 0 Loss = 0.08 Acc = 89.22\n",
      "Epoch = 3 iter = 50 Loss = 0.07 Acc = 89.52\n",
      "Epoch = 3 iter = 100 Loss = 0.08 Acc = 89.78\n",
      "Epoch = 3 iter = 150 Loss = 0.07 Acc = 90.04\n",
      "Epoch = 3 iter = 200 Loss = 0.05 Acc = 90.31\n",
      "Epoch = 3 iter = 250 Loss = 0.05 Acc = 90.57\n",
      "Epoch = 3 iter = 300 Loss = 0.04 Acc = 90.81\n",
      "Epoch = 3 iter = 350 Loss = 0.05 Acc = 91.02\n",
      "Epoch = 3 iter = 400 Loss = 0.07 Acc = 91.22\n",
      "Epoch = 3 iter = 450 Loss = 0.05 Acc = 91.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/468 [00:00<00:18, 24.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:18, 25.00it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4 iter = 0 Loss = 0.03 Acc = 91.51\n",
      "Epoch = 4 iter = 50 Loss = 0.05 Acc = 91.68\n",
      "Epoch = 4 iter = 100 Loss = 0.06 Acc = 91.84\n",
      "Epoch = 4 iter = 150 Loss = 0.06 Acc = 91.99\n",
      "Epoch = 4 iter = 200 Loss = 0.07 Acc = 92.13\n",
      "Epoch = 4 iter = 250 Loss = 0.06 Acc = 92.27\n",
      "Epoch = 4 iter = 300 Loss = 0.03 Acc = 92.42\n",
      "Epoch = 4 iter = 350 Loss = 0.04 Acc = 92.56\n",
      "Epoch = 4 iter = 400 Loss = 0.07 Acc = 92.68\n",
      "Epoch = 4 iter = 450 Loss = 0.06 Acc = 92.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/468 [00:00<00:17, 26.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:17, 27.43it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5 iter = 0 Loss = 0.05 Acc = 92.84\n",
      "Epoch = 5 iter = 50 Loss = 0.05 Acc = 92.96\n",
      "Epoch = 5 iter = 100 Loss = 0.05 Acc = 93.07\n",
      "Epoch = 5 iter = 150 Loss = 0.04 Acc = 93.19\n",
      "Epoch = 5 iter = 200 Loss = 0.02 Acc = 93.31\n",
      "Epoch = 5 iter = 250 Loss = 0.04 Acc = 93.42\n",
      "Epoch = 5 iter = 300 Loss = 0.05 Acc = 93.52\n",
      "Epoch = 5 iter = 350 Loss = 0.05 Acc = 93.61\n",
      "Epoch = 5 iter = 400 Loss = 0.05 Acc = 93.69\n",
      "Epoch = 5 iter = 450 Loss = 0.03 Acc = 93.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/468 [00:00<00:19, 23.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:18, 25.45it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6 iter = 0 Loss = 0.04 Acc = 93.81\n",
      "Epoch = 6 iter = 50 Loss = 0.03 Acc = 93.9\n",
      "Epoch = 6 iter = 100 Loss = 0.06 Acc = 93.98\n",
      "Epoch = 6 iter = 150 Loss = 0.03 Acc = 94.07\n",
      "Epoch = 6 iter = 200 Loss = 0.02 Acc = 94.16\n",
      "Epoch = 6 iter = 250 Loss = 0.02 Acc = 94.24\n",
      "Epoch = 6 iter = 300 Loss = 0.05 Acc = 94.31\n",
      "Epoch = 6 iter = 350 Loss = 0.04 Acc = 94.38\n",
      "Epoch = 6 iter = 400 Loss = 0.03 Acc = 94.45\n",
      "Epoch = 6 iter = 450 Loss = 0.03 Acc = 94.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/468 [00:00<00:23, 19.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:18, 26.16it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7 iter = 0 Loss = 0.02 Acc = 94.55\n",
      "Epoch = 7 iter = 50 Loss = 0.02 Acc = 94.63\n",
      "Epoch = 7 iter = 100 Loss = 0.02 Acc = 94.7\n",
      "Epoch = 7 iter = 150 Loss = 0.03 Acc = 94.76\n",
      "Epoch = 7 iter = 200 Loss = 0.03 Acc = 94.82\n",
      "Epoch = 7 iter = 250 Loss = 0.04 Acc = 94.88\n",
      "Epoch = 7 iter = 300 Loss = 0.04 Acc = 94.93\n",
      "Epoch = 7 iter = 350 Loss = 0.05 Acc = 94.97\n",
      "Epoch = 7 iter = 400 Loss = 0.04 Acc = 95.03\n",
      "Epoch = 7 iter = 450 Loss = 0.02 Acc = 95.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/468 [00:00<00:18, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:18, 25.76it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8 iter = 0 Loss = 0.02 Acc = 95.11\n",
      "Epoch = 8 iter = 50 Loss = 0.03 Acc = 95.16\n",
      "Epoch = 8 iter = 100 Loss = 0.03 Acc = 95.21\n",
      "Epoch = 8 iter = 150 Loss = 0.02 Acc = 95.26\n",
      "Epoch = 8 iter = 200 Loss = 0.02 Acc = 95.32\n",
      "Epoch = 8 iter = 250 Loss = 0.02 Acc = 95.36\n",
      "Epoch = 8 iter = 300 Loss = 0.02 Acc = 95.41\n",
      "Epoch = 8 iter = 350 Loss = 0.02 Acc = 95.46\n",
      "Epoch = 8 iter = 400 Loss = 0.03 Acc = 95.51\n",
      "Epoch = 8 iter = 450 Loss = 0.02 Acc = 95.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/468 [00:00<00:20, 22.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:19, 24.31it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9 iter = 0 Loss = 0.01 Acc = 95.57\n",
      "Epoch = 9 iter = 50 Loss = 0.05 Acc = 95.6\n",
      "Epoch = 9 iter = 100 Loss = 0.03 Acc = 95.65\n",
      "Epoch = 9 iter = 150 Loss = 0.01 Acc = 95.69\n",
      "Epoch = 9 iter = 200 Loss = 0.04 Acc = 95.72\n",
      "Epoch = 9 iter = 250 Loss = 0.02 Acc = 95.77\n",
      "Epoch = 9 iter = 300 Loss = 0.02 Acc = 95.8\n",
      "Epoch = 9 iter = 350 Loss = 0.03 Acc = 95.84\n",
      "Epoch = 9 iter = 400 Loss = 0.03 Acc = 95.87\n",
      "Epoch = 9 iter = 450 Loss = 0.02 Acc = 95.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/468 [00:00<00:15, 30.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:18, 26.04it/s]                         \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "dataset = MelDataset(BASEDIR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "accuracy = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    dataloader = MelDataset.batchify(dataset, 32)\n",
    "    \n",
    "    # training\n",
    "    for i, (mels, speakers, accents, input_lengths) in enumerate(dataloader):\n",
    "        mels = mels.to(device)\n",
    "        speakers = speakers.to(device)\n",
    "        accents = accents.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        h_n, logits = model(mels, input_lengths)\n",
    "        loss = loss_func(logits, speakers).mean()\n",
    "        accuracy.append(sum(torch.argmax(logits, dim=1) == speakers).item() * 100. / len(accents))\n",
    "    \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch = {epoch} iter = {i} Loss = {round(np.array(losses).mean(), 2)} Acc = {round(np.array(accuracy).mean(), 2)}\")\n",
    "            losses = []\n",
    "            \n",
    "    print(\"Extracting Embedding\")\n",
    "    dataloader = MelDataset.batchify(dataset, 32)\n",
    "    metadata_speaker = []\n",
    "    metadata_accent = []   \n",
    "    embeddings = []\n",
    "\n",
    "    for i, (mels, speakers, accents, input_lengths) in tqdm(enumerate(dataloader), total=len(dataset) // 32):\n",
    "\n",
    "        metadata_speaker += speakers.numpy().tolist()\n",
    "        metadata_accent += accents.numpy().tolist()\n",
    "        mels = mels.to(device)\n",
    "        speakers = speakers.to(device)\n",
    "        accents = accents.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            h_n, logits = model(mels, input_lengths)\n",
    "            embeddings.append(h_n.cpu())\n",
    "\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    writer.add_embedding(tag=\"accent\",\n",
    "                        mat=embeddings,\n",
    "                        global_step=epoch,\n",
    "                        metadata=metadata_accent)\n",
    "    writer.add_embedding(tag=\"speaker\",\n",
    "            mat=embeddings,\n",
    "            global_step=epoch,\n",
    "            metadata=metadata_speaker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
