{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from src.dataset_hrg import getDataLoader\n",
    "from src.module_hrg import EmbeddingHRG\n",
    "from torch_geometric.data.batch import Batch\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifier\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DurationClassifier(torch.nn.Module):\n",
    "    def __init__(self, h_dim, n_buckets):\n",
    "        super(DurationClassifier, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.gcn = EmbeddingHRG(data_tr.dataset.n_vocab, self.h_dim, self.h_dim).cuda()\n",
    "        self.n_buckets = n_buckets\n",
    "        self.h_dim = h_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        self.cls_head = nn.Sequential(nn.Linear(self.h_dim, self.h_dim // 2),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(self.h_dim // 2, self.n_buckets))\n",
    "    def forward(self, x, durations):\n",
    "        x = self.gcn(x)\n",
    "        bsz, max_syll_nodes, h_dim = x.shape\n",
    "        duration_labels = self.get_duration_label_tensor(durations, max_syll_nodes)\n",
    "        logits = self.cls_head(x)\n",
    "        loss = self.criterion(logits.view(-1, self.n_buckets), duration_labels.view(-1))\n",
    "        return loss, logits\n",
    "    \n",
    "    def get_duration_label_tensor(self, durations, max_syll_nodes):\n",
    "        syll_dur = torch.zeros(len(durations), max_syll_nodes).long() - 1\n",
    "        for i in range(len(durations)):\n",
    "            syll_dur[i, :len(durations[i])] = torch.LongTensor(durations[i])\n",
    "        return syll_dur.to(self.device)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"config/config-arctic.yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "_config = config['solver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = getDataLoader(\n",
    "    mode='train',\n",
    "    meta_path=_config['meta_path']['train'],\n",
    "    data_dir=_config['data_dir'],\n",
    "    batch_size=_config['batch_size'],\n",
    "    r=config['model']['tacotron']['r'],\n",
    "    n_jobs=_config['n_jobs'],\n",
    "    use_gpu=True,\n",
    "    add_info_headers=[\"dur\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_buckets = 11\n",
    "h_dim = 256\n",
    "label_dict = {f\"DB_{i}\": i for i in range(n_buckets)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Init the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_classifier = DurationClassifier(h_dim=h_dim, n_buckets=n_buckets).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(dur_classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_duration_sequence(label_seq):\n",
    "    label_seq = [ls.strip().split() for ls in label_seq]\n",
    "    res = []\n",
    "    for ls in label_seq:\n",
    "        res.append([label_dict[x] for x in ls])\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(data_tr, 0):\n",
    "        _, txt, text_lengths, mel, spec, durations = batch\n",
    "        durations = parse_duration_sequence(durations)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss, logits = dur_classifier(txt, durations)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = getDataLoader(\n",
    "    mode='test',\n",
    "    meta_path=_config['meta_path']['test'],\n",
    "    data_dir=_config['data_dir'],\n",
    "    batch_size=_config['batch_size'],\n",
    "    r=config['model']['tacotron']['r'],\n",
    "    n_jobs=_config['n_jobs'],\n",
    "    use_gpu=True,\n",
    "    vocab=data_tr.dataset.vocab,\n",
    "    add_info_headers=[\"dur\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_classifier = dur_classifier.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(data_test, 0):\n",
    "        _, txt, text_lengths, mel, spec, durations = batch\n",
    "        durations = parse_duration_sequence(durations)\n",
    "        loss, logits = dur_classifier(txt, durations)\n",
    "        break\n",
    "dur_classifier = dur_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_accuracy(self, durations_true, logits):\n",
    "    durations_predicted = logits.argmax(-1)\n",
    "    durations_predicted = durations_predicted.detach().cpu().numpy()\n",
    "    num, den = 0., 0.\n",
    "    for i, d in enumerate(durations_predicted):\n",
    "        num += (durations_predicted[i, :len(durations_true[i])] == durations_true[i]).sum()\n",
    "        den += len(durations_true[i])\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_duration_accuracy(durations_true=durations, durations_predicted=logits.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_predicted=logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_predicted = durations_predicted.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations_predicted[0, :len(durations[0])] == np.array(durations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = pad_durations(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_all = pd.read_csv(\"arctic/meta_all.txt\", sep=\"|\", names=[\"mel\", \"lin\", \"len\", \"hrg\", \"add_info\"])\n",
    "meta_all = meta_all.drop(labels=\"add_info\", axis=1)\n",
    "syll_dur = pd.read_csv(\"syll_dur_end.txt\", sep=\"|\", names=[\"source\", \"target\", \"end\"])\n",
    "merged = pd.concat([meta_all, syll_dur], axis=1)\n",
    "merged[[\"mel\", \"lin\", \"len\", \"hrg\", \"target\"]].to_csv(\"arctic/meta_all.txt\", sep=\"|\", header=False, index=None)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
